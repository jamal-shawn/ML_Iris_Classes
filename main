import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Data download from sklearn
from sklearn.datasets import load_iris
data=load_iris().data
target=load_iris().target
df_data=pd.DataFrame(data,columns=['sepal_length','sepal_width','petal_length','petal_width'])
df_target=pd.DataFrame(target,columns=['target'])

df_data.info() # does not include class?!?!?
df_target.info()
data_file = pd.read_csv("iris.data",names=['sepal_length','sepal_width','petal_length','petal_width','class'])
df = pd.merge(data_file,df_target, how="inner", left_index=True, right_index=True)

df.info()
print(df.head())
print(df.groupby('class').mean())
print(df.groupby('class').std())

# when sorting by classes, it appears that the peta and sepal lengths/widths are very different based on average and std. dev.
# can probably determine class from sepal and petal characteristics.
fig1 = sns.lmplot(x="sepal_width",
                 y="sepal_length",
                 hue="class",
                 data=df)
fig2 = sns.lmplot(x="petal_width",
                 y="petal_length",
                 hue="class",
                 data=df)
                 
Y=df["class"]
X=df[["sepal_length","sepal_width","petal_length","petal_width"]].to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test) 

print("accuracy: ",accuracy_score(y_test, y_pred))
pred_True = (y_pred == y_test).sum()
pred_False = (y_pred != y_test).sum()

print("True ", pred_True)
print("False ", pred_False)
print(y_pred.shape)

k_series = [1, 3, 5, 7, 10, 20, 30, 40, 50]
trials=10
acc = np.zeros((len(k_series),trials))
pred = np.zeros((len(k_series),trials))
for index_1, k in enumerate(k_series):
    for index_2, i in enumerate(range(trials)):
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
        knn_clf = KNeighborsClassifier(n_neighbors=k)
        knn_clf.fit(X_train, y_train)
        y_pred = knn_clf.predict(X_test)
        acc[index_1,index_2] = accuracy_score(y_test, y_pred)
        pred[index_1,index_2]= (y_pred == y_test).sum()
    
acc_avg = np.mean(acc,axis=1)
pred_corr_avg = np.mean(pred,axis=1)

knn_avg = pd.DataFrame(index=k_series,columns=['acc_avg','pred_corr_avg','k value'])
knn_avg.acc_avg,knn_avg.pred_corr_avg,knn_avg["k value"]=acc_avg,pred_corr_avg,k_series

fig3 = sns.lmplot(x="k value",
                 y="acc_avg",
                 data=knn_avg)
#plot looks parabolic, not linear. Try parabolic lmplot
fig4 = sns.lmplot(x="k value",
                  y="acc_avg",
                  data=knn_avg,
                  order=2)

knn_avg

#appears that there is a peak at 5-15 in accuracy. I will run it again, with a different k series and trial number. 
#However it does appear that k has an impact on accuracy of the model. Too small or too large of a K effect accuracy. There is
# a "goldilocks" zone.k_series = np.linspace(1,30,30,dtype=int)
trials=100
acc = np.zeros((len(k_series),trials))
pred = np.zeros((len(k_series),trials))
for index_1, k in enumerate(k_series):
    for index_2, i in enumerate(range(trials)):
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
        knn_clf = KNeighborsClassifier(n_neighbors=k)
        knn_clf.fit(X_train, y_train)
        y_pred = knn_clf.predict(X_test)
        acc[index_1,index_2] = accuracy_score(y_test, y_pred)
        pred[index_1,index_2]= (y_pred == y_test).sum()
    
acc_avg = np.mean(acc,axis=1)
pred_corr_avg = np.mean(pred,axis=1)

knn_avg = pd.DataFrame(index=k_series,columns=['acc_avg','pred_corr_avg','k value'])
knn_avg.acc_avg,knn_avg.pred_corr_avg,knn_avg["k value"]=acc_avg,pred_corr_avg,k_series

fig3 = sns.lmplot(x="k value",
                 y="acc_avg",
                 data=knn_avg)
fig4 = sns.lmplot(x="k value",
                  y="acc_avg",
                  data=knn_avg,
                  order=2)

knn_avg
